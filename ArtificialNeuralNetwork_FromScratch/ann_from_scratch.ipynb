{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.metrics as mt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datasets.make_moons(n_samples = 500, noise = 0.05)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'x_1': x[:,0], 'x_2': x[:,1], 'y': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = np.unique(y, return_counts=True)\n",
    "for label, qt_label in zip(unique[0], unique[1]):\n",
    "    print(f'Label: {label}\\t Counts: {qt_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:,0], x[:,1], c = y, s = 50, alpha=0.5, cmap= 'cool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "- Weights and Bias Inicialization\n",
    "- Feedfoward\n",
    "- Loss calculation\n",
    "- Backpropagation\n",
    "- Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NnModel:\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, hidden_neurons: int = 10, output_neurons: int = 2):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.input_neurons = self.x.shape[1]\n",
    "        \n",
    "        # Weights and Bias Inicialization\n",
    "        # Xavier Inialization -> equal variation in all layers\n",
    "        \n",
    "        self.W1 = np.random.randn(self.input_neurons, self.hidden_neurons) / np.sqrt(self.input_neurons)\n",
    "        self.B1 = np.zeros((1, self.hidden_neurons))\n",
    "        \n",
    "        self.W2 = np.random.randn(self.hidden_neurons, self.output_neurons) / np.sqrt(self.hidden_neurons)\n",
    "        self.B2 = np.zeros((1, self.output_neurons))\n",
    "\n",
    "        self.model_dict = {'W1': self.W1, 'B1': self.B1, 'W2': self.W2, 'B2': self.B2}\n",
    "        \n",
    "        #Z and activation function (1)\n",
    "        self.z1 = 0\n",
    "        self.f1 = 0\n",
    "    \n",
    "    def foward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Line Equation Z (1)\n",
    "        self.z1 = x.dot(self.W1) + self.B1\n",
    "        \n",
    "        # Activation Function (1)\n",
    "        self.f1 = np.tanh(self.z1)\n",
    "        \n",
    "        # Line Equation Z (2)\n",
    "        z2 = self.f1.dot(self.W2) + self.B2\n",
    "        \n",
    "        # Softmax\n",
    "        exp_values = np.exp(z2)\n",
    "        softmax = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        \n",
    "        return softmax\n",
    "    \n",
    "    def loss(self, softmax):\n",
    "        # Cross Entropy\n",
    "        predictions = np.zeros(self.y.shape[0])\n",
    "        \n",
    "        for i, correct_index in enumerate(self.y):\n",
    "            predicted = softmax[i][correct_index]\n",
    "            predictions[i] = predicted\n",
    "        \n",
    "        log_prob = -np.log(predicted)\n",
    "        \n",
    "        return log_prob / self.y.shape[0]\n",
    "    \n",
    "    def backpropagation(self, softmax: np.ndarray, learning_rate: float) -> None :\n",
    "        delta2 = np.copy(softmax)\n",
    "        \n",
    "        delta2[range(self.x.shape[0]), self.y] -= 1\n",
    "        \n",
    "        dW2 = (self.f1.T).dot(delta2)\n",
    "        dB2 = np.sum(delta2, axis = 0, keepdims = True)\n",
    "        \n",
    "        delta1 = delta2.dot(self.W2.T)*(1-np.power(np.tanh(self.z1), 2))\n",
    "        dW1 = (self.x.T).dot(delta1)\n",
    "        dB1 = np.sum(delta1, axis = 0, keepdims = True)\n",
    "        \n",
    "        # Weight update\n",
    "        self.W1 += - learning_rate * dW1\n",
    "        self.W2 += - learning_rate * dW2\n",
    "        self.B1 += - learning_rate * dB1\n",
    "        self.B2 += - learning_rate * dB2\n",
    "    \n",
    "    # NOT NEEDED - functions shows plots of the learning\n",
    "    def show_plot(self, predictions):\n",
    "        if self.x.shape[1] == 2:\n",
    "            plt.scatter(self.x[:,0], self.x[:,1], s=50,c=predictions, cmap='cool', alpha=0.7)\n",
    "            plt.show()\n",
    "        elif self.x.shape[1] == 3:\n",
    "            ax = plt.axes(projection='3d')\n",
    "            ax.scatter3D(self.x[:,0], self.x[:,1], self.x[:,2], s=50,c=predictions, cmap='cool', alpha=0.7)\n",
    "        \n",
    "    def fit(self, epochs: int, lr: float):\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "           outputs = self.foward(self.x)\n",
    "           loss = self.loss(outputs)\n",
    "           self.backpropagation(outputs, lr)\n",
    "           \n",
    "           # Accuracy\n",
    "           prediction = np.argmax(outputs, axis=1)\n",
    "           correct = (prediction == self.y).sum()\n",
    "           accuracy = correct/self.y.shape[0]\n",
    "           \n",
    "           if int((epoch+1) % (epochs/10)) == 0:\n",
    "               print(f'Epoch: [{epoch+1} / {epochs}] Accuracy: {accuracy:.3f} Loss: {loss.item():.4f}')\n",
    "               self.show_plot(prediction)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 10\n",
    "output_neurons = 2\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "model = NnModel(x, y, hidden_neurons, output_neurons)\n",
    "result = model.fit(epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = datasets.make_blobs(n_samples = 400, n_features=2, centers=4, random_state=10, cluster_std=1.5, shuffle=True)\n",
    "\n",
    "#plt.scatter(x_[:,0], x_[:,1], s=50,c=y_, cmap='cool', alpha=0.7)\n",
    "\n",
    "hidden_neurons = 8\n",
    "output_neurons = 4\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "model = NnModel(x_, y_, hidden_neurons, output_neurons)\n",
    "result = model.fit(epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = datasets.make_blobs(n_samples = 400, n_features=3, centers=4, random_state=50, cluster_std=0.9, shuffle=True)\n",
    "\n",
    "hidden_neurons = 10\n",
    "output_neurons = 4\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "modelo_3d = NnModel(x_, y_, hidden_neurons, output_neurons)\n",
    "resultado_3d = modelo_3d.fit(epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.load_breast_cancer(as_frame=True)\n",
    "\n",
    "x_bc = df.data.to_numpy()[:, 4:9]\n",
    "y_bc = df.target.to_numpy()\n",
    "\n",
    "bc = NnModel(x_bc, y_bc, 5, 2)\n",
    "result_bc = bc.fit(2000, 0.001)\n",
    "\n",
    "confusion_matrix = mt.confusion_matrix(y_bc, result_bc)\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap='coolwarm', fmt='.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Models and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modelo_treinado.pickle', 'wb') as file:\n",
    "    pickle.dump(bc, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modelo_treinado.pickle', 'rb') as file:\n",
    "    modelo_carregado = pickle.load(file)\n",
    "\n",
    "modelo_carregado.foward(np.array([0.085,0.064,0.01,0.0099, 0.078])).argmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
